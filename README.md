
# Informer - Time Series Forecasting

This project implements the **Informer** deep learning architecture from scratch to perform **long-sequence time series forecasting** on real-world energy data (ETT-small dataset).

---

## ðŸ“¦ Project Structure

```
informer_timeseries/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ ETT-small.csv
â”‚
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ attention.py
â”‚   â”œâ”€â”€ encoder.py
â”‚   â”œâ”€â”€ decoder.py
â”‚   â”œâ”€â”€ informer.py
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ runs/
â”‚   â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ evaluation/
â”‚   â””â”€â”€ testing/
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ replot_training_loss.py
â”‚
â”œâ”€â”€ train.py
â”œâ”€â”€ evaluate.py
â”œâ”€â”€ test.py
â””â”€â”€ README.md
```

---

## ðŸ“ˆ Training Details

- Dataset: **ETT-small** (Electricity Transformer Temperature)
- Input sequence length: `96`
- Label length: `48`
- Prediction length: `24`
- Model size: `d_model=512`, `n_heads=8`, `e_layers=3`
- Training epochs: `50`
- Optimizer: `Adam`
- Learning rate scheduler: `StepLR (step_size=10, gamma=0.5)`
- Device: `GPU (NVIDIA GeForce RTX 3050)`

---

## ðŸ”¥ Results

| Metric | Score |
|:-------|:------|
| Test MAE | 1.0684 |
| Test RMSE | 1.4173 |

âœ… Successfully learned seasonality and trend patterns from time series data.

---

## ðŸ“Š Outputs

- ðŸ“ˆ Training Loss Curve: `/runs/training/training_loss_plot.png`
- ðŸ“ˆ Training vs Validation Loss Curve: `/runs/training/training_vs_validation_loss_plot.png`
- ðŸ“„ Evaluation MAE/RMSE: `/runs/evaluation/evaluation_results.txt`
- ðŸ“ˆ Evaluation Sample Predictions: `/runs/evaluation/sample_predictions.png`
- ðŸ“ˆ Final Forecasts: `/runs/testing/forecast_plot.png`

---

## ðŸš€ How to Run

1. Install dependencies:

```bash
pip install -r requirements.txt
```

2. Train model:

```bash
python train.py
```

3. Evaluate on test set:

```bash
python evaluate.py
```

4. Forecast future data:

```bash
python test.py
```

---

## ðŸ›  Future Improvements

- Add MinMax normalization preprocessing
- Train longer with fine-tuned learning rates

- ---

## ðŸ“š Reference

This project is an implementation inspired by the research paper:

> **Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting**  
> Authors: Haoyiet al.  
> [Link to Paper (arXiv)](https://arxiv.org/abs/2012.07436)

Please refer to the original paper for theoretical foundations and complete architectural insights.

---

- Test other attention mechanisms
- Try larger d_model and deeper encoder layers

---

âœ… Built completely from scratch, including attention, encoder, decoder, and training loops!
